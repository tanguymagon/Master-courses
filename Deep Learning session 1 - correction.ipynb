{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oEyHhHF39Bui"
   },
   "source": [
    "## Usage\n",
    "Like a regular notebook:\n",
    "- __run a cell__ with **_ctrl-enter_** or **_shift-enter_**\n",
    "- __use the command palette__ with **_ctrl-shift-P_** to find more complex commands\n",
    "\n",
    "Use it only with __Chrome__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2394,
     "status": "ok",
     "timestamp": 1571386884741,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "oy8-Z4EV2nQm",
    "outputId": "4e84d5e2-1b18-4ddf-e919-d81f3f705d8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PowerTransformer, MinMaxScaler\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwCsLx1L47iW"
   },
   "source": [
    "# Session 1\n",
    "\n",
    "This introduction will altern theory & practice, and will be divided as follow:\n",
    "0. Data Exploration \n",
    "1. Problematic \n",
    "2. From Text to numbers\n",
    "    1. From one text to a list of tokens \n",
    "    2. From a list of tokens to an embedding \n",
    "3. Modelling \n",
    "    1. Baseline model \n",
    "    2. Logistic Regression\n",
    "4. Neural Network\n",
    "    1. Architecture \n",
    "    2. Parameters and initialization \n",
    "    3. Activation functions  \n",
    "    4. Forward propagation  \n",
    "    5. Measuring progress\n",
    "    6. Computing gradients with backpropagation\n",
    "    7. Updating parameters - teaching\n",
    "    8. Assembling bricks - training\n",
    "    9. Letâ€™s try\n",
    "        1. Archi 1\n",
    "        2. Archi 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z175aef45DLp"
   },
   "source": [
    "## 0. Data exploration\n",
    "\n",
    "Let's download datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 83835,
     "status": "ok",
     "timestamp": 1571386972615,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "_NtCfm105Eas",
    "outputId": "96f60a9a-3616-47fb-9f86-0f885db0cbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ..  test_set.csv\n",
      "Downloading ..  test_set_with_rating.csv\n",
      "Downloading ..  train_set.csv\n"
     ]
    }
   ],
   "source": [
    "def download_data():\n",
    "    import urllib.request\n",
    "    import os\n",
    "    folder = './data'\n",
    "    list_urls = [\n",
    "        ('https://bendrive.s3-eu-west-1.amazonaws.com/test_set.csv', 'test_set.csv'),\n",
    "        ('https://bendrive.s3-eu-west-1.amazonaws.com/test_set_with_rating.csv', 'test_set_with_rating.csv'),\n",
    "        ('https://bendrive.s3-eu-west-1.amazonaws.com/train_set.csv', 'train_set.csv')\n",
    "        \n",
    "    ]\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    for url, file in list_urls:\n",
    "        if not os.path.isfile(folder + '/' + file):\n",
    "            print(\"Downloading .. \", file)\n",
    "            urllib.request.urlretrieve(url, folder + '/' + file)\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Wk1Gd9t5H8V"
   },
   "source": [
    "With pandas, load the datasets into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qz3-y46M5LkD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_test_w_rating = pd.read_csv(\"./data/test_set_with_rating.csv\")\n",
    "df_test = pd.read_csv(\"./data/test_set.csv\")\n",
    "df_train = pd.read_csv(\"./data/train_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6z_JlWRH5Nt8"
   },
   "source": [
    "Load only a part of the train data for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1571386998567,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "R-fSwdjO5ONU",
    "outputId": "3be7e504-a588-4de5-e484-a2aed1706add"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>useful_review</th>\n",
       "      <th>funny_review</th>\n",
       "      <th>cool_review</th>\n",
       "      <th>name_restaurant</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>review_count_restaurant</th>\n",
       "      <th>is_open</th>\n",
       "      <th>name_user</th>\n",
       "      <th>review_count_user</th>\n",
       "      <th>yelping_since</th>\n",
       "      <th>useful_user</th>\n",
       "      <th>funny_user</th>\n",
       "      <th>cool_user</th>\n",
       "      <th>elite</th>\n",
       "      <th>fans</th>\n",
       "      <th>average_stars</th>\n",
       "      <th>compliment_hot</th>\n",
       "      <th>compliment_more</th>\n",
       "      <th>compliment_profile</th>\n",
       "      <th>compliment_cute</th>\n",
       "      <th>compliment_list</th>\n",
       "      <th>compliment_note</th>\n",
       "      <th>compliment_plain</th>\n",
       "      <th>compliment_cool</th>\n",
       "      <th>compliment_funny</th>\n",
       "      <th>compliment_photos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like walking back in time, every Saturday morn...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>FIk4lQQu1eTe2EpzQ4xhBA</td>\n",
       "      <td>8mIrX_LrOnAqWsB5JrOojQ</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Pinball Hall Of Fame</td>\n",
       "      <td>1610 E Tropicana Ave</td>\n",
       "      <td>Las Vegas</td>\n",
       "      <td>NV</td>\n",
       "      <td>89119.0</td>\n",
       "      <td>36.101449</td>\n",
       "      <td>-115.130511</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Carol</td>\n",
       "      <td>866.0</td>\n",
       "      <td>2010-08-26 22:09:14</td>\n",
       "      <td>1635.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>726.0</td>\n",
       "      <td>2011,2012,2013,2014,2015,2016,2017,2018</td>\n",
       "      <td>63.0</td>\n",
       "      <td>4.16</td>\n",
       "      <td>18.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  ...  compliment_photos\n",
       "0  Like walking back in time, every Saturday morn...  ...               15.0\n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train[0:30000]\n",
    "df_train.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZMr4tvT5QOM"
   },
   "source": [
    "We will focus on review and rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 508,
     "status": "ok",
     "timestamp": 1571387001799,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "rrb2docl5Sk6",
    "outputId": "e2f9c3f3-86eb-4d19-f47e-38e5b7d2c07b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOD REVIEW\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12073</th>\n",
       "      <td>Excellent food!!!!! Masala Dosa is legit!!!  Don't believe the other reviews... this is so good, flavors are on point and just spicy enough!</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             review  rating\n",
       "12073  Excellent food!!!!! Masala Dosa is legit!!!  Don't believe the other reviews... this is so good, flavors are on point and just spicy enough!  5     "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAD REVIEW\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3657</th>\n",
       "      <td>The guy who took my order was really rude. It was my first time going there so I guess I didn't order quick enough or I wasn't an expert in whatever I was ordering. I wanted a mixed berry smoothie at first and the guy had the nerve to roll his eyes in my face and tell me he just didn't have yogurt or whatever to make it. Then I was like ok, I want a caramel macchiato. He mumbled something under his breath, so thinking that he couldn't make it, I started to say \"ok then I'll have something else.\" He bit back with \"NO! Do you want iced or heated?!\" I said iced. I also wanted a bagel; I looked him straight in the eyes and ordered my bagel while he was gossiping with someone else and he never gave me my bagel xS, never even processed that I had ordered something else. Not going back there again.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  review  rating\n",
       "3657  The guy who took my order was really rude. It was my first time going there so I guess I didn't order quick enough or I wasn't an expert in whatever I was ordering. I wanted a mixed berry smoothie at first and the guy had the nerve to roll his eyes in my face and tell me he just didn't have yogurt or whatever to make it. Then I was like ok, I want a caramel macchiato. He mumbled something under his breath, so thinking that he couldn't make it, I started to say \"ok then I'll have something else.\" He bit back with \"NO! Do you want iced or heated?!\" I said iced. I also wanted a bagel; I looked him straight in the eyes and ordered my bagel while he was gossiping with someone else and he never gave me my bagel xS, never even processed that I had ordered something else. Not going back there again.  1     "
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "df_train = df_train[['review', 'rating']].dropna()\n",
    "df_train['rating'] = df_train['rating'].astype('int')\n",
    "\n",
    "# Show a good review\n",
    "print(\"GOOD REVIEW\")\n",
    "display(df_train[df_train['rating']==5].sample(1))\n",
    "\n",
    "# Show a bad review\n",
    "print(\"BAD REVIEW\")\n",
    "display(df_train[df_train['rating']==1].sample(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nmUESo65VGT"
   },
   "source": [
    "Let's look at the number of comments for each rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4394,
     "status": "ok",
     "timestamp": 1571387010273,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "cdYKTl0Q5XWh",
    "outputId": "10717222-5291-43a2-89ae-4a8933f3eb58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5    12483\n",
      "4    7372 \n",
      "3    3972 \n",
      "1    3498 \n",
      "2    2669 \n",
      "Name: rating, dtype: int64\n",
      "5    0.416183\n",
      "4    0.245782\n",
      "3    0.132426\n",
      "1    0.116623\n",
      "2    0.088984\n",
      "Name: rating, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(df_train['rating'].value_counts())\n",
    "print(df_train['rating'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1juKwcl95ZRC"
   },
   "source": [
    "Let's look at the distribution in a more convenient way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2392,
     "status": "ok",
     "timestamp": 1571387013895,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "4trsy2zW5bRM",
    "outputId": "bf20de36-e143-42b4-f403-459a7e648093"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d35df6240>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "df_train[\"rating\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5xN2YHS5dG3"
   },
   "source": [
    "# 1. Problematic\n",
    "\n",
    "Are we able to predict the rating from the text comment ?\n",
    "\n",
    "So our goal is to have a model, a function, that takes text as input and output a rating.\n",
    "\n",
    "$f(text) = rating$\n",
    "\n",
    "Are **you** able to do it ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1571387020243,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "lZr9fic35ftY",
    "outputId": "7e7a74c5-d4f0-406c-d661-0f3d09b02eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GUESS THE RATING ?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28377    The vibe is good and the staff is friendly but the food is mediocre. We ordered the brisket nachos but the meat had no flavor. The jalapenos were freshly sliced and the tomatoes are fresh but you have to pick up each ingredient and place it on a chip in order to build a chip with any flavor. It was an interesting display when serving the food but beyond the initial novelty, the taste is just not there.\n",
       "Name: review, dtype: object"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a bad review\n",
    "import numpy as np\n",
    "\n",
    "print(\"GUESS THE RATING ?\")\n",
    "random_rating = np.random.randint(1, 6)\n",
    "display(df_train[df_train['rating']==random_rating]['review'].sample(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1274,
     "status": "ok",
     "timestamp": 1571387023579,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "2bkCgPfb5l5O",
    "outputId": "f98d70da-767a-4817-a1a6-1d6969969570"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(random_rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2hBW7xg75n7h"
   },
   "source": [
    "To begin with a binary classification problem, we will bin the target into bad review and good review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4650,
     "status": "ok",
     "timestamp": 1571387030579,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "4adKDkHc5ocv",
    "outputId": "0233b50b-46a1-4f87-8498-fc453f81c2be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2d34ad5390>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAF+tJREFUeJzt3X+wXPV53/H3JxC7LrZjHJI7CiIV\nmcqZYmiJuYPppE2vS4Jl2rGcNuPCOEbY1Ipj6CQt00ZOMoPH1DN2E5wZXBdXrjWCloBp/ANNwCUK\n9Q5JJ3KAmCLAcbhgHKTIqEEO5NopiZynf+z3umudK91l92pXV/f9mtm5Z5/zPed8Hwn0uXvO2d1U\nFZIkDfquaU9AknTiMRwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVKH4SBJ6jh12hMY1Rln\nnFEbNmwYadtvfOMbnHbaaSs7oROcPa8N9nzyG7ffBx988E+r6vuWG7dqw2HDhg088MADI23b6/WY\nm5tb2Qmd4Ox5bbDnk9+4/Sb56jDjPK0kSeowHCRJHYaDJKnDcJAkdSwbDknOSvL5JI8leTTJz7X6\nq5PsTvJ4+3l6qyfJjUnmkzyc5HUD+9rSxj+eZMtA/YIke9s2NybJ8WhWkjScYV45HAaurapzgIuA\nq5OcA2wD7q2qjcC97TnAm4CN7bEVuAn6YQJcB7weuBC4bjFQ2ph3DWy3afzWJEmjWjYcqupAVf1B\nW/5z4EvAmcBm4OY27GbgLW15M3BL9e0BXpVkHfBGYHdVHaqqrwO7gU1t3Surak/1v5buloF9SZKm\n4EW9zyHJBuBHgC8AM1V1oK36GjDTls8Enh7YbF+rHau+b4n6UsffSv/VCDMzM/R6vRcz/W9bWFgY\nedvVyp7XBns++U2q36HDIcnLgU8BP19Vzw9eFqiqSnLcv4y6qrYD2wFmZ2dr1DeCrLU3zYA9rxX2\nfPKbVL9DhUOS76YfDLdW1adb+Zkk66rqQDs1dLDV9wNnDWy+vtX2A3NH1Hutvn6J8ZJ0wtqw7a6p\nHHfnpsl8VMgwdysF+ATwpar68MCqXcDiHUdbgDsH6le0u5YuAp5rp5/uAS5Jcnq7EH0JcE9b93yS\ni9qxrhjYlyRpCoZ55fCjwNuBvUkearVfBD4I3JHkKuCrwFvburuBS4F54JvAOwCq6lCS64H727j3\nV9WhtvweYCfwMuBz7SFJmpJlw6Gqfhc42vsOLl5ifAFXH2VfO4AdS9QfAM5dbi6SpMnwHdKSpA7D\nQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwk\nSR2GgySpw3CQJHUYDpKkjmG+Q3pHkoNJHhmofTLJQ+3x1OLXhybZkOQvBtZ9bGCbC5LsTTKf5Mb2\nfdEkeXWS3Ukebz9PPx6NSpKGN8wrh53ApsFCVf2Lqjq/qs4HPgV8emD1E4vrqurdA/WbgHcBG9tj\ncZ/bgHuraiNwb3suSZqiZcOhqu4DDi21rv32/1bgtmPtI8k64JVVtad9x/QtwFva6s3AzW355oG6\nJGlKTh1z+38IPFNVjw/Uzk7yReB54Jer6neAM4F9A2P2tRrATFUdaMtfA2aOdrAkW4GtADMzM/R6\nvZEmvbCwMPK2q5U9rw32PDnXnnd44seEyfU7bjhczne+ajgA/GBVPZvkAuCzSV477M6qqpLUMdZv\nB7YDzM7O1tzc3EiT7vV6jLrtamXPa4M9T86V2+6a+DEBdm46bSL9jhwOSU4F/hlwwWKtql4AXmjL\nDyZ5AngNsB9YP7D5+lYDeCbJuqo60E4/HRx1TpKklTHOraw/DvxhVX37dFGS70tySlv+IfoXnp9s\np42eT3JRu05xBXBn22wXsKUtbxmoS5KmZJhbWW8Dfg/44ST7klzVVl1G90L0jwEPt1tbfwN4d1Ut\nXsx+D/BfgHngCeBzrf5B4CeSPE4/cD44Rj+SpBWw7Gmlqrr8KPUrl6h9iv6trUuNfwA4d4n6s8DF\ny81DkjQ5vkNaktRhOEiSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lS\nh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1DHM14TuSHIwySMDtfcl2Z/kofa4dGDde5PMJ/ly\nkjcO1De12nySbQP1s5N8odU/meQlK9mgJOnFG+aVw05g0xL1X6uq89vjboAk59D/bunXtm3+U5JT\nkpwCfBR4E3AOcHkbC/Chtq+/DXwduOrIA0mSJmvZcKiq+4BDQ+5vM3B7Vb1QVV8B5oEL22O+qp6s\nqr8Ebgc2Jwnwj4HfaNvfDLzlRfYgSVphp46x7TVJrgAeAK6tqq8DZwJ7BsbsazWAp4+ovx74XuDP\nqurwEuM7kmwFtgLMzMzQ6/VGmvjCwsLI265W9rw22PPkXHve4eUHHQeT6nfUcLgJuB6o9vMG4J0r\nNamjqartwHaA2dnZmpubG2k/vV6PUbddrex5bbDnybly210TPybAzk2nTaTfkcKhqp5ZXE7yceA3\n29P9wFkDQ9e3GkepPwu8Ksmp7dXD4HhJ0pSMdCtrknUDT38SWLyTaRdwWZKXJjkb2Aj8PnA/sLHd\nmfQS+hetd1VVAZ8HfqptvwW4c5Q5SZJWzrKvHJLcBswBZyTZB1wHzCU5n/5ppaeAnwGoqkeT3AE8\nBhwGrq6qb7X9XAPcA5wC7KiqR9shfgG4Pcm/B74IfGLFupMkjWTZcKiqy5coH/Uf8Kr6APCBJep3\nA3cvUX+S/t1MkqQThO+QliR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgO\nkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUsGw5JdiQ5mOSRgdqvJPnDJA8n+UyS\nV7X6hiR/keSh9vjYwDYXJNmbZD7JjUnS6q9OsjvJ4+3n6cejUUnS8IZ55bAT2HREbTdwblX9XeCP\ngPcOrHuiqs5vj3cP1G8C3gVsbI/FfW4D7q2qjcC97bkkaYqWDYequg84dETtt6rqcHu6B1h/rH0k\nWQe8sqr2VFUBtwBvaas3Aze35ZsH6pKkKTl1BfbxTuCTA8/PTvJF4Hngl6vqd4AzgX0DY/a1GsBM\nVR1oy18DZo52oCRbga0AMzMz9Hq9kSa8sLAw8rarlT2vDfY8Odeed3j5QcfBpPodKxyS/BJwGLi1\nlQ4AP1hVzya5APhsktcOu7+qqiR1jPXbge0As7OzNTc3N9K8e70eo267Wtnz2mDPk3PltrsmfkyA\nnZtOm0i/I4dDkiuBfwpc3E4VUVUvAC+05QeTPAG8BtjPd556Wt9qAM8kWVdVB9rpp4OjzkmStDJG\nupU1ySbg3wFvrqpvDtS/L8kpbfmH6F94frKdNno+yUXtLqUrgDvbZruALW15y0BdkjQly75ySHIb\nMAeckWQfcB39u5NeCuxud6TuaXcm/Rjw/iR/Bfw18O6qWryY/R76dz69DPhcewB8ELgjyVXAV4G3\nrkhnkqSRLRsOVXX5EuVPHGXsp4BPHWXdA8C5S9SfBS5ebh6SpMnxHdKSpA7DQZLUYThIkjoMB0lS\nh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR2GgySpw3CQJHUY\nDpKkjqHCIcmOJAeTPDJQe3WS3Ukebz9Pb/UkuTHJfJKHk7xuYJstbfzjSbYM1C9Isrdtc2P7nmlJ\n0pQM+8phJ7DpiNo24N6q2gjc254DvAnY2B5bgZugHyb0v3/69cCFwHWLgdLGvGtguyOPJUmaoGW/\nQxqgqu5LsuGI8mZgri3fDPSAX2j1W6qqgD1JXpVkXRu7u6oOASTZDWxK0gNeWVV7Wv0W4C3A50Zt\najl79z/HldvuOl67P6qnPvhPJn5MSRrFONccZqrqQFv+GjDTls8Enh4Yt6/VjlXft0RdkjQlQ71y\nWE5VVZJaiX0dS5Kt9E9VMTMzQ6/XG2k/My+Da887vIIzG86o810JCwsLUz3+NNjz2jCtnqfxbwhM\nrt9xwuGZJOuq6kA7bXSw1fcDZw2MW99q+/n/p6EW671WX7/E+I6q2g5sB5idna25ubmlhi3rI7fe\nyQ17VyQXX5Sn3jY38WMu6vV6jPrntVrZ89owrZ6ncWoaYOem0ybS7zinlXYBi3ccbQHuHKhf0e5a\nugh4rp1+uge4JMnp7UL0JcA9bd3zSS5qdyldMbAvSdIUDPXrc5Lb6P/Wf0aSffTvOvogcEeSq4Cv\nAm9tw+8GLgXmgW8C7wCoqkNJrgfub+Pev3hxGngP/TuiXkb/QvRxuxgtSVresHcrXX6UVRcvMbaA\nq4+ynx3AjiXqDwDnDjMXSdLx5zukJUkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeow\nHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR0jh0OSH07y0MDj+SQ/n+R9\nSfYP1C8d2Oa9SeaTfDnJGwfqm1ptPsm2cZuSJI1nqK8JXUpVfRk4HyDJKcB+4DP0vzP616rqVwfH\nJzkHuAx4LfADwG8neU1b/VHgJ4B9wP1JdlXVY6POTZI0npHD4QgXA09U1VeTHG3MZuD2qnoB+EqS\neeDCtm6+qp4ESHJ7G2s4SNKUrNQ1h8uA2waeX5Pk4SQ7kpzeamcCTw+M2ddqR6tLkqYkVTXeDpKX\nAH8CvLaqnkkyA/wpUMD1wLqqemeS/wjsqar/1rb7BPC5tptNVfUvW/3twOur6poljrUV2AowMzNz\nwe233z7SnA8eeo5n/mKkTcdy3pnfM/mDNgsLC7z85S+f2vGnwZ7Xhmn1vHf/cxM/JsDZ33PKWP2+\n4Q1veLCqZpcbtxKnld4E/EFVPQOw+BMgyceB32xP9wNnDWy3vtU4Rv07VNV2YDvA7Oxszc3NjTTh\nj9x6JzfsXakzasN76m1zEz/mol6vx6h/XquVPa8N0+r5ym13TfyYADs3nTaRflfitNLlDJxSSrJu\nYN1PAo+05V3AZUlemuRsYCPw+8D9wMYkZ7dXIZe1sZKkKRnr1+ckp9G/y+hnBsr/Icn59E8rPbW4\nrqoeTXIH/QvNh4Grq+pbbT/XAPcApwA7qurRceYlSRrPWOFQVd8AvveI2tuPMf4DwAeWqN8N3D3O\nXCRJK8d3SEuSOgwHSVKH4SBJ6jAcJEkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeow\nHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6xg6HJE8l2ZvkoSQPtNqrk+xO8nj7eXqrJ8mNSeaT\nPJzkdQP72dLGP55ky7jzkiSNbqVeObyhqs6vqtn2fBtwb1VtBO5tzwHeBGxsj63ATdAPE+A64PXA\nhcB1i4EiSZq843VaaTNwc1u+GXjLQP2W6tsDvCrJOuCNwO6qOlRVXwd2A5uO09wkSctYiXAo4LeS\nPJhka6vNVNWBtvw1YKYtnwk8PbDtvlY7Wl2SNAWnrsA+/kFV7U/y/cDuJH84uLKqKkmtwHFo4bMV\nYGZmhl6vN9J+Zl4G1553eCWm9KKMOt+VsLCwMNXjT4M9rw3T6nka/4bA5PodOxyqan/7eTDJZ+hf\nM3gmybqqOtBOGx1sw/cDZw1svr7V9gNzR9R7SxxrO7AdYHZ2tubm5o4cMpSP3HonN+xdiVx8cZ56\n29zEj7mo1+sx6p/XamXPa8O0er5y210TPybAzk2nTaTfsU4rJTktySsWl4FLgEeAXcDiHUdbgDvb\n8i7ginbX0kXAc+300z3AJUlObxeiL2k1SdIUjPvr8wzwmSSL+/r1qvofSe4H7khyFfBV4K1t/N3A\npcA88E3gHQBVdSjJ9cD9bdz7q+rQmHOTJI1orHCoqieBv7dE/Vng4iXqBVx9lH3tAHaMMx9J0srw\nHdKSpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAkdRgOkqQOw0GS1GE4SJI6DAdJUofh\nIEnqMBwkSR2GgySpw3CQJHUYDpKkjpHDIclZST6f5LEkjyb5uVZ/X5L9SR5qj0sHtnlvkvkkX07y\nxoH6plabT7JtvJYkSeMa52tCDwPXVtUfJHkF8GCS3W3dr1XVrw4OTnIOcBnwWuAHgN9O8pq2+qPA\nTwD7gPuT7Kqqx8aYmyRpDCOHQ1UdAA605T9P8iXgzGNsshm4vapeAL6SZB64sK2bb99HTZLb21jD\nQZKmZEWuOSTZAPwI8IVWuibJw0l2JDm91c4Enh7YbF+rHa0uSZqScU4rAZDk5cCngJ+vqueT3ARc\nD1T7eQPwznGP0461FdgKMDMzQ6/XG2k/My+Da887vBJTelFGne9KWFhYmOrxp8Ge14Zp9TyNf0Ng\ncv2OFQ5Jvpt+MNxaVZ8GqKpnBtZ/HPjN9nQ/cNbA5utbjWPUv0NVbQe2A8zOztbc3NxI8/7IrXdy\nw96xc/FFe+ptcxM/5qJer8eof16rlT2vDdPq+cptd038mAA7N502kX7HuVspwCeAL1XVhwfq6waG\n/STwSFveBVyW5KVJzgY2Ar8P3A9sTHJ2kpfQv2i9a9R5SZLGN86vzz8KvB3Ym+ShVvtF4PIk59M/\nrfQU8DMAVfVokjvoX2g+DFxdVd8CSHINcA9wCrCjqh4dY16SpDGNc7fS7wJZYtXdx9jmA8AHlqjf\nfaztJEmT5TukJUkdhoMkqcNwkCR1GA6SpA7DQZLUYThIkjoMB0lSh+EgSeowHCRJHYaDJKnDcJAk\ndRgOkqQOw0GS1GE4SJI6DAdJUofhIEnqMBwkSR0nTDgk2ZTky0nmk2yb9nwkaS07IcIhySnAR4E3\nAefQ/x7qc6Y7K0lau06IcAAuBOar6smq+kvgdmDzlOckSWvWiRIOZwJPDzzf12qSpCk4ddoTeDGS\nbAW2tqcLSb484q7OAP50ZWY1vHxo0kf8DlPpecrseW1YUz2/4UNj9/u3hhl0ooTDfuCsgefrW+07\nVNV2YPu4B0vyQFXNjruf1cSe1wZ7PvlNqt8T5bTS/cDGJGcneQlwGbBrynOSpDXrhHjlUFWHk1wD\n3AOcAuyoqkenPC1JWrNOiHAAqKq7gbsndLixT02tQva8NtjzyW8i/aaqJnEcSdIqcqJcc5AknUBO\n6nBY7iM5krw0ySfb+i8k2TD5Wa6sIXr+N0keS/JwknuTDHVb24ls2I9eSfLPk1SSVX1nyzD9Jnlr\n+3t+NMmvT3qOK22I/65/MMnnk3yx/bd96TTmuZKS7EhyMMkjR1mfJDe2P5OHk7xuRSdQVSflg/6F\n7SeAHwJeAvxv4JwjxrwH+Fhbvgz45LTnPYGe3wD8zbb8s2uh5zbuFcB9wB5gdtrzPs5/xxuBLwKn\nt+ffP+15T6Dn7cDPtuVzgKemPe8V6PvHgNcBjxxl/aXA54AAFwFfWMnjn8yvHIb5SI7NwM1t+TeA\ni5NkgnNcacv2XFWfr6pvtqd76L+nZDUb9qNXrgc+BPzfSU7uOBim33cBH62qrwNU1cEJz3GlDdNz\nAa9sy98D/MkE53dcVNV9wKFjDNkM3FJ9e4BXJVm3Usc/mcNhmI/k+PaYqjoMPAd870Rmd3y82I8h\nuYr+bx6r2bI9t5fbZ1XVXZOc2HEyzN/xa4DXJPlfSfYk2TSx2R0fw/T8PuCnk+yjf9fjv5rM1Kbq\nuH7s0AlzK6smK8lPA7PAP5r2XI6nJN8FfBi4cspTmaRT6Z9amqP/yvC+JOdV1Z9NdVbH1+XAzqq6\nIcnfB/5rknOr6q+nPbHV6mR+5TDMR3J8e0ySU+m/HH12IrM7Pob6GJIkPw78EvDmqnphQnM7Xpbr\n+RXAuUAvyVP0z83uWsUXpYf5O94H7Kqqv6qqrwB/RD8sVqther4KuAOgqn4P+Bv0P3PpZDbU/++j\nOpnDYZiP5NgFbGnLPwX8z2pXelapZXtO8iPAf6YfDKv9XDQs03NVPVdVZ1TVhqraQP86y5ur6oHp\nTHdsw/x3/Vn6rxpIcgb900xPTnKSK2yYnv8YuBggyd+hHw7/Z6KznLxdwBXtrqWLgOeq6sBK7fyk\nPa1UR/lIjiTvBx6oql3AJ+i//Jynf+HnsunNeHxD9vwrwMuB/96uvf9xVb15apMe05A9nzSG7Pce\n4JIkjwHfAv5tVa3aV8RD9nwt8PEk/5r+xekrV/kveiS5jX7In9GupVwHfDdAVX2M/rWVS4F54JvA\nO1b0+Kv8z0+SdByczKeVJEkjMhwkSR2GgySpw3CQJHUYDpKkDsNBktRhOEiSOgwHSVLH/wMPT10T\nmtTvTAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['bin_rating'] = df_train['rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "df_train[\"bin_rating\"].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uwz2ON-5qkS"
   },
   "source": [
    "# 2. From text to numbers\n",
    "\n",
    "First step is to transform text into numbers, to go from:\n",
    "\n",
    "    f(text) = rating\n",
    "\n",
    "to\n",
    "\n",
    "    f(numerical_vector_representing_text) = rating\n",
    "\n",
    "For example:\n",
    "\n",
    "    \"Meh.  Nothing special.  Food was just ok.\"\n",
    "\n",
    "must be transformed to something like:\n",
    "\n",
    "    [0, 4, 2, 8]\n",
    "\n",
    "where the values of the vector convey meaning of the text. Two different texts must have a different representation:\n",
    "\n",
    "    \"Meh.  Nothing special.  Food was just ok.\" > [0, 4, 2, 8]\n",
    "    \n",
    "    \"Aww.  Spectacular.  Food was amaaaaazing.\" > [4, 2, 6, 1]\n",
    "\n",
    "Also, it would be more convenient if texts of different sizes can translate to a fixed size vector, for example:\n",
    "\n",
    "    \"Meh.  Nothing special.  Food was just ok.\" > [0, 4, 2, 8]\n",
    "    \n",
    "    \"Meh.\" > [0, 3, 3, 7]\n",
    "\n",
    "\n",
    "## 2.1 From one text to a list of tokens\n",
    "\n",
    "First step is to convert a text, which is one string of characters, to a list of words.\n",
    "\n",
    "    \"Meh.  Nothing special.\" > [\"meh\", \"nothing\", \"special\"]\n",
    "    \n",
    "Then, those words must be transformed into tokens, wich are representation of words. This can be done with\n",
    "\n",
    "(1) **Stemming** is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n",
    "\n",
    "(2) **Lemmatization** is related to stemming, differing in that lemmatization is able to capture canonical forms based on a word's lemma.\n",
    "\n",
    "(3) **Everything else**: set all characters to **lowercase**, remove numbers (or convert numbers to textual representations), remove **punctuation** (generally part of tokenization, but still worth keeping in mind at this stage, even as confirmation), strip white space (also generally part of tokenization), remove default  **stop words**  (general English stop words), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vz2KQGP-5uZR"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_lowercase(df, colname):\n",
    "    df[colname] = df[colname].str.lower()\n",
    "    return df\n",
    "    \n",
    "def not_regex(pattern):\n",
    "        return r\"((?!{}).)\".format(pattern)\n",
    "\n",
    "def remove_punctuation(df, colname):\n",
    "    df[colname] = df[colname].str.replace('\\n', ' ')\n",
    "    df[colname] = df[colname].str.replace('\\r', ' ')\n",
    "    alphanumeric_characters_extended = '(\\\\b[-/]\\\\b|[a-zA-Z0-9])'\n",
    "    df[colname] = df[colname].str.replace(not_regex(alphanumeric_characters_extended), ' ')\n",
    "    return df\n",
    "\n",
    "def tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].str.split()\n",
    "    return df\n",
    "\n",
    "def remove_stop_words(df, colname):\n",
    "    stop_words = stopwords.words('english')\n",
    "    df[colname] = df[colname].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    return df\n",
    "\n",
    "def reverse_tokenize_sentence(df, colname):\n",
    "    df[colname] = df[colname].map(lambda word: ' '.join(word))\n",
    "    return df\n",
    "\n",
    "\n",
    "def text_cleaning(df, colname):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. convert text to lowercase\n",
    "    2. remove punctuation and new line characters '\\n'\n",
    "    3. Tokenize sentences\n",
    "    4. Remove all stopwords\n",
    "    5. convert tokenized text to text\n",
    "    \"\"\"\n",
    "    df = (\n",
    "        df\n",
    "        .pipe(convert_text_to_lowercase, colname)\n",
    "        .pipe(remove_punctuation, colname)\n",
    "        .pipe(tokenize_sentence, colname)\n",
    "        .pipe(remove_stop_words, colname)\n",
    "        .pipe(reverse_tokenize_sentence, colname)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ykt6PWd5zdS"
   },
   "source": [
    "- clean the review column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12331,
     "status": "ok",
     "timestamp": 1571387073900,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "_sNUjm0y533I",
    "outputId": "382fd198-efc8-45a2-e82b-fc19244a906c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>bin_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17799</th>\n",
       "      <td>greenberg gets full five stars book yes know authentic jewish deli foods pretty close especially given casino las vegas reuben world similar reuben 1/2 pound larger carnegie deli new york 24 reuben 10 nearly good stop brisket turkey meats good pickles mustards whole nine yards n none additional open breakfast hours full menu available nice also 5 95 deal either bacon egg cheese croissant pastrami egg bagel coupled large coffee good deal croissant huge n nreally like place especially compare areas nearby 10 ok crepe 6 00 so-so slice pizza etc good food good value</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         review  ...  bin_rating\n",
       "17799  greenberg gets full five stars book yes know authentic jewish deli foods pretty close especially given casino las vegas reuben world similar reuben 1/2 pound larger carnegie deli new york 24 reuben 10 nearly good stop brisket turkey meats good pickles mustards whole nine yards n none additional open breakfast hours full menu available nice also 5 95 deal either bacon egg cheese croissant pastrami egg bagel coupled large coffee good deal croissant huge n nreally like place especially compare areas nearby 10 ok crepe 6 00 so-so slice pizza etc good food good value  ...  1         \n",
       "\n",
       "[1 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = text_cleaning(df_train, 'review')\n",
    "\n",
    "df_cleaned.sample(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BeNjzzuL56Av"
   },
   "source": [
    "## 2.2. From a list of tokens to an embedding (numerical vector)\n",
    "\n",
    "We still have to transform the list of tokens to a fixed size numerical vector\n",
    "\n",
    "    [\"meh\", \"nothing\", \"special\"] > [0, 2, 8, 1]\n",
    "    [\"my\", \"belly\", \"is\", \"happy\"] > [2, 1, 9, 3]\n",
    "    [\"i\", \"took\", \"a\", \"loan\", \"to\", \"eat\", \"there\"] > [1, 1, 3, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0nP5wq2p6APd"
   },
   "source": [
    "For that, we will use a very common technique called TF-IDF\n",
    "\n",
    "**Definition**\n",
    "\n",
    "> **TF-IDF** or **term frequencyâ€“inverse document frequency**, is a numerical statistic that is intended to reflect how important a word is to reflect how important a word is to a document in a collection or corpus.\n",
    "\n",
    "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization:  \n",
    "      \n",
    "    TF(t) = (Nbr of times term t appears in a document) / (Total nbr of terms in the document)\n",
    "    \n",
    "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following:  \n",
    "      \n",
    "    IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
    "\n",
    "_Example:_\n",
    "_Consider a document containing 100 words wherein the word _cat_ appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word _cat_ appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12._\n",
    "\n",
    "**Tools**\n",
    "\n",
    "Python **sklearn** package **feature_extraction**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lpdWvc3U6A57"
   },
   "outputs": [],
   "source": [
    "number_of_dimensions = 64\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    analyzer='word',\n",
    "    ngram_range=(1, 1),\n",
    "    max_features=number_of_dimensions,\n",
    "    max_df=1.0,\n",
    "    min_df=10)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df_cleaned[\"review\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1571387086752,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "6dXZ4_vG6DlZ",
    "outputId": "8c6f861e-70b0-484b-ff39-cef51a1daa5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " == Text\n",
      "\n",
      "went last weekend pretty disappointed one thing pictured recommended yelp good started steak grilled skewers ok nothing special freind got lasagna got special chicken dish pretty bland lacking kick waitress really nice got manger switch dishes freind got hamburger got shrimp scampi hamburger better lasagna still lacking flavor scampi better chicken also still seasoned noodles bit cooked big name attached restaurant going empty stomach high hopes service great gave three stars\n",
      "\n",
      " == Rating\n",
      "\n",
      "2\n",
      "\n",
      " == Embedding\n",
      "\n",
      "[0.14294182 0.         0.         0.         0.         0.\n",
      " 0.32447549 0.         0.         0.         0.30544986 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09921426 0.73213226 0.1089112  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.14805736 0.         0.\n",
      " 0.12663703 0.         0.         0.         0.         0.\n",
      " 0.32825946 0.13462864 0.14619609 0.         0.         0.10883736\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1618795  0.        ]\n"
     ]
    }
   ],
   "source": [
    "randrow = np.random.randint(100)\n",
    "\n",
    "print(\" == Text\\n\")\n",
    "print(df_cleaned[\"review\"][randrow])\n",
    "print(\"\\n == Rating\\n\")\n",
    "print(df_cleaned[\"rating\"][randrow])\n",
    "print(\"\\n == Embedding\\n\")\n",
    "print(X_tfidf[randrow])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrmO42vr6Hkm"
   },
   "source": [
    "## 3. Modelling\n",
    "\n",
    "Yeah ! We now have a numerical representation of the text, and the rating associated. Remember we start with a binary classification task: bad review / good review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e_-Fb-wl6IHT"
   },
   "source": [
    "#### Split data into train / test sets\n",
    "\n",
    "In order to measure the performance of our modeling, we need to split the data into:\n",
    "\n",
    "- The **training set** contains a known output and the model learns on this data in order to be generalized to other data later on (= 80%)\n",
    "\n",
    "- We have the **test set** in order to test our modelâ€™s prediction on this test set (= 20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjp-bX_66MUM"
   },
   "outputs": [],
   "source": [
    "TARGET = 'bin_rating'\n",
    "FEATURE = 'review'\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_cleaned[FEATURE], \n",
    "    df_cleaned[TARGET], \n",
    "    test_size=0.2,\n",
    "    random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_YH1WLD96M_p"
   },
   "source": [
    "## 3.1 Baseline model\n",
    "\n",
    "As a baseline model, let's use a random model with a probability of good/bad review based on the distribution of the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ztwy8Cu56PTq"
   },
   "outputs": [],
   "source": [
    "y_pred_baseline = np.random.choice(\n",
    "    [0, 1],\n",
    "    size=len(y_test),\n",
    "    p=[1-y_train.mean(), y_train.mean()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PaIsN2hS6Rim"
   },
   "source": [
    "We need now to evaluate our baseline model.\n",
    "\n",
    "- **True Positives** (TP): these are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. *E.g. if actual class value indicates that this passenger survived and predicted class tells you the same thing.*\n",
    "\n",
    "- **True Negatives** (TN): these are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. *E.g. if actual class says this passenger did not survive and predicted class tells you the same thing.*\n",
    "\n",
    "*False positives and false negatives, these values occur when your actual class contradicts with the predicted class.*\n",
    "\n",
    "- **False Positives** (FP): when actual class is no and predicted class is yes. *E.g. if actual class says this passenger did not survive but predicted class tells you that this passenger will survive.*\n",
    "\n",
    "- **False Negatives** (FN): when actual class is yes but predicted class in no. *E.g. if actual class value indicates that this passenger survived and predicted class tells you that passenger will die.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x20PqpFj6UIK"
   },
   "source": [
    "- **Accuracy**: Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model.\n",
    "\n",
    "> Accuracy = TP+TN/TP+FP+FN+TN\n",
    "\n",
    "*= 154656 + 68551 / 154656 + 14186 + 9438 +68551 = 0.90*\n",
    "\n",
    "- **Precision**: Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate.\n",
    "\n",
    "> Precision = TP/TP+FP\n",
    "\n",
    "*= 154656 / 154656 + 14186 = 0.92*\n",
    "\n",
    "- **Recall (Sensitivity)**: Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. The question recall answers is: Of all the passengers that truly survived, how many did we label?\n",
    "\n",
    "> Recall = TP/TP+FN\n",
    "\n",
    "*= 154656 / 154656 + 9438 = 0.94*\n",
    "\n",
    "- **F1 score**: F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, itâ€™s better to look at both Precision and Recall.\n",
    "\n",
    "> F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "*= 2 * (0.94 * 0.92) / (0.94 + 0.92) = 0.93*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9217,
     "status": "ok",
     "timestamp": 1571387109222,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "CuObK8aL6ZT2",
    "outputId": "057ef4c8-54a7-441e-944d-f87bd226a997"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 631, 1370],\n",
       "       [1299, 2699]])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred_baseline, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1787,
     "status": "ok",
     "timestamp": 1571387116269,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "hte1zGa16bei",
    "outputId": "d08d03b2-22eb-49ef-88a1-f91d733f9599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5550925154192365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.32      0.32      2001\n",
      "           1       0.66      0.68      0.67      3998\n",
      "\n",
      "    accuracy                           0.56      5999\n",
      "   macro avg       0.50      0.50      0.50      5999\n",
      "weighted avg       0.55      0.56      0.55      5999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy {}'.format(accuracy_score(y_pred_baseline, y_test)))\n",
    "print(classification_report(y_test, y_pred_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F8sS4caP6b9-"
   },
   "source": [
    "## 3.2 Logistic Regression\n",
    "\n",
    "We can do better than a random model with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sDaRgm46efH"
   },
   "source": [
    "#### Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3432,
     "status": "ok",
     "timestamp": 1571387123349,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "5oi7IBc56glf",
    "outputId": "c36555a1-f4d9-483c-e8d3-f8b302e2e76e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=64,\n",
       "                                 min_df=10, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, use_idf=True,\n",
       "                                 vocabulary=None)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=None, solver='lbfgs',\n",
       "                                    tol=0.0001, verbose=2, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(solver='lbfgs', verbose=2, n_jobs=-1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', tfidf_vectorizer),\n",
    "    ('model', logit)])\n",
    "\n",
    "pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pWam65Tj6j8V"
   },
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtZD2-Sz6nBR"
   },
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4075,
     "status": "ok",
     "timestamp": 1571387137578,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "6z-nzDm46pSF",
    "outputId": "90625ed1-7f67-4237-b090-7abe26399411"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1205,  796],\n",
       "       [ 545, 3453]])"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2560,
     "status": "ok",
     "timestamp": 1571387140867,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "7gZwCJTY6rHk",
    "outputId": "ad57dc76-b2c0-4f32-e67f-c8bd77577ee7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7764627437906317\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.60      0.64      2001\n",
      "           1       0.81      0.86      0.84      3998\n",
      "\n",
      "    accuracy                           0.78      5999\n",
      "   macro avg       0.75      0.73      0.74      5999\n",
      "weighted avg       0.77      0.78      0.77      5999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy {}'.format(accuracy_score(y_pred, y_test)))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IT4RiNew6tQe"
   },
   "source": [
    "Fortunately, logistic regression can do better than a random model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c5onek-f6v52"
   },
   "source": [
    "# 4. Neural Network\n",
    "\n",
    "## 4.1. Architecture\n",
    "\n",
    "Now, let's try to design a neural network model for our task.\n",
    "\n",
    "* What would be the input layer size ?\n",
    "* What would be the output layer size ?\n",
    "* What would be the output layer activation function ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RbBU79JE6zAH"
   },
   "source": [
    "We store the architecture of the neural network in a list of dictionnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QdDy61Y46zz0"
   },
   "outputs": [],
   "source": [
    "NN_ARCHITECTURE_0 = [\n",
    "    {\"input_dim\": number_of_dimensions, \"output_dim\": 8, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 8, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "NN_ARCHITECTURE_1 = [\n",
    "    {\"input_dim\": number_of_dimensions, \"output_dim\": 32, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 32, \"output_dim\": 16, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 16, \"output_dim\": 8, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 8, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCikVxpz64mp"
   },
   "source": [
    "## 4.2. Parameters and initialization\n",
    "\n",
    "Each neuron has two parameters: weight (W) and bias (b).\n",
    "\n",
    "Let's initialize those parameters for each neuron and store them in a dictionnary with notation:\n",
    "* Wi is the array of the weights of the neurons of layer i\n",
    "* bi is the array of the biases of the neurons of the layer i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2943,
     "status": "ok",
     "timestamp": 1571387152376,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "RVaFsP5-65LN",
    "outputId": "fb877849-ed95-4169-c6b9-ff15cacf18d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (8, 64)\n",
      "b1 (8, 1)\n",
      "W2 (4, 8)\n",
      "b2 (4, 1)\n",
      "W3 (1, 4)\n",
      "b3 (1, 1)\n"
     ]
    }
   ],
   "source": [
    "def init_layers(nn_architecture, seed = 99):\n",
    "    # random seed initiation\n",
    "    np.random.seed(seed)\n",
    "    # number of layers in our neural network\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    # parameters storage initiation\n",
    "    params_values = {}\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        \n",
    "        # extracting the number of units in layers\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        # initiating the values of the W matrix\n",
    "        # and vector b for subsequent layers\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values\n",
    "\n",
    "params_values = init_layers(NN_ARCHITECTURE_0)\n",
    "\n",
    "for key, value in params_values.items():\n",
    "    print(key, value.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hb61uTDG6-36"
   },
   "source": [
    "## 4.3. Activation functions\n",
    "\n",
    "What does a neuron do ?\n",
    "\n",
    "First it computes a value: Z, the linear combination of its inputs A by its weighted coefficients W and add a bias b. Then it applies an activation function to Z, and generates an output A.\n",
    "\n",
    "$$\\boldsymbol{Z}^{[l]} = \\boldsymbol{W}^{[l]} \\cdot \\boldsymbol{A}^{[l-1]} + \\boldsymbol{b}^{[l]}$$\n",
    "\n",
    "$$\\boldsymbol{A}^{[l]} = g^{[l]}(\\boldsymbol{Z}^{[l]})$$\n",
    "\n",
    "So, imagine a network where input layer has 3 neurons (layer 0), and next layer (layer 1) has 2 neurons.\n",
    "\n",
    "    N00\n",
    "            N10  \n",
    "    N01\n",
    "            N11\n",
    "    N02\n",
    "\n",
    "N10 would compute a value, like this:\n",
    "\n",
    "$$Z^{[10]} = W^{[10-00]} * A^{[00]} + W^{[10-01]} * A^{[01]} + W^{[10-02]} * A^{[02]} + b^{[10]}$$\n",
    "$$A^{[10]} = g^{[l]}(Z^{[10]})$$\n",
    "\n",
    "* What is the role of the activation function ?\n",
    "* Code the activation functions sigmoid and relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxXqsEPz6_cH"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 694,
     "status": "ok",
     "timestamp": 1571387161085,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "QsnSMwpV7BXj",
    "outputId": "4ec1a591-8072-4966-acd0-9bcf116660a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2d32217b00>]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndxGRCFK4i4hEkMJdRCSC/j81Q8E/+n+3bgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 20)\n",
    "plt.plot(x, sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1571387165743,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "vWDxwaif7Eb7",
    "outputId": "b4e8a849-3941-4887-a7d5-fbd5cfdcbf18"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2d3087b080>]"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(-10, 20)\n",
    "plt.plot(x, relu(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zddwzgzt7G00"
   },
   "source": [
    "## 4.3. Forward propagation\n",
    "\n",
    "Forward propagation of a neural network is the left to right pass. It is the inference of the neural network model: from the input layer, the output layer is computed by forward propagation the value from one neuron to another.\n",
    "\n",
    "With the stored weights and biases of the neutwork, let's write the forward propagation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "st8uP0mQ7Hf6"
   },
   "outputs": [],
   "source": [
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    # calculation of the input value for the activation function\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    # return of calculated activation A and the intermediate Z matrix\n",
    "    return activation_func(Z_curr), Z_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h04NdH5e7J9a"
   },
   "outputs": [],
   "source": [
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    # creating a temporary memory to store the information needed for a backward step\n",
    "    memory = {}\n",
    "    # X vector is the activation for layer 0â€Š\n",
    "    A_curr = X\n",
    "    \n",
    "    # iteration over network layers\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        # we number network layers from 1\n",
    "        layer_idx = idx + 1\n",
    "        # transfer the activation from the previous iteration\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        # extraction of W for the current layer\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        # extraction of b for the current layer\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        # calculation of activation for the current layer\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        # saving calculated values in the memory\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    # return of prediction vector and a dictionary containing intermediate values\n",
    "    return A_curr, memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLym6eI07MHU"
   },
   "source": [
    "Let's try with a random observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1943,
     "status": "ok",
     "timestamp": 1571387180864,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "3Aow_EMf7OWB",
    "outputId": "fce40c69-7b06-4f41-e2dd-8404a2cf1400"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output value is: [[0.49529866]]\n"
     ]
    }
   ],
   "source": [
    "random_input = np.random.randn(number_of_dimensions, 1)\n",
    "output, _ = full_forward_propagation(random_input, params_values, NN_ARCHITECTURE_0)\n",
    "print(\"Output value is:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YsXVh9IW7SSs"
   },
   "source": [
    "Let's now try with our train data embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3208,
     "status": "ok",
     "timestamp": 1571387189764,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "-LnJ75k_7S2V",
    "outputId": "0170b31a-678b-4c90-8a49-81c9a5d7b2c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23995, 64)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_features = tfidf_vectorizer.fit_transform(x_train).toarray()\n",
    "x_train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5144,
     "status": "ok",
     "timestamp": 1571387196538,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "-dvdilA17U0b",
    "outputId": "613eaf37-4230-41b4-b5b0-87bd2d06ffd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 23995)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output, _ = full_forward_propagation(np.transpose(x_train_features), params_values, NN_ARCHITECTURE_0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1197,
     "status": "ok",
     "timestamp": 1571387217259,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "0D1D6JkW7X-D",
    "outputId": "e208d2a3-2939-4280-c1db-09cb3cde396e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23995,)"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ts3KMcdA7aBT"
   },
   "source": [
    "So, we are able to infer a neural net to our data ! But are we good ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1109,
     "status": "ok",
     "timestamp": 1571387220055,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "muPcrRXk7ca7",
    "outputId": "0eb418bf-10a2-416e-ae33-b159ad8d0373"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49113446 0.49245251 0.49245251 ... 0.49245251 0.49245251 0.49245251]]\n"
     ]
    }
   ],
   "source": [
    "print(output[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DI2JgTdv7c6v"
   },
   "source": [
    "The output is now a proba, we need to convert it to a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcUtAAFU7e2q"
   },
   "outputs": [],
   "source": [
    "def convert_prob_into_class(probs):\n",
    "    probs_ = np.copy(probs)\n",
    "    probs_[probs_ > 0.5] = 1\n",
    "    probs_[probs_ <= 0.5] = 0\n",
    "    return probs_\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1276,
     "status": "ok",
     "timestamp": 1571387229481,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "hOIDsztH7ibJ",
    "outputId": "beec5427-77c2-4c2e-a81e-fb81ee771c3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_first_inference = convert_prob_into_class(output)\n",
    "print(y_pred_first_inference[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tl8juFkG7k7u"
   },
   "source": [
    "So how does it look ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1571387233245,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "dVJMUTll7m6k",
    "outputId": "940dc318-7b85-4f5e-e28d-bab212d357b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.3391539904146697\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.34      0.51     23995\n",
      "         1.0       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.34     23995\n",
      "   macro avg       0.50      0.17      0.25     23995\n",
      "weighted avg       1.00      0.34      0.51     23995\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('accuracy {}'.format(accuracy_score(np.squeeze(y_pred_first_inference), y_train)))\n",
    "print(classification_report(np.squeeze(y_pred_first_inference), y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKks8-Y-7pTV"
   },
   "source": [
    "BAD !\n",
    "\n",
    "Wait a minute, we forgot to train the neural network !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5U7fY9BU7poz"
   },
   "source": [
    "## 4.4 Measuring progress\n",
    "\n",
    "Evaluating with a classification report is nice for the human to understand the model performances. But in order to train the model, we need a simpler and computational measure off the model fitness.\n",
    "\n",
    "This measure is the **loss**. The loss can be different from the metric that we - as human - use to evaluate the model.\n",
    "\n",
    "We should routinely calculate the value of the loss function. \"Generally speaking, the loss function is designed to show how far we are from the 'ideal' solution.\" It is selected according to the problem we plan to solve. For binary classification, binary crossentropy is common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsB-0QMD7r5-"
   },
   "outputs": [],
   "source": [
    "def get_cost_value(Y_hat, Y):\n",
    "    # number of examples\n",
    "    m = Y_hat.shape[1]\n",
    "    # calculation of the cost according to the formula\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3969,
     "status": "ok",
     "timestamp": 1571387750657,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "3Kqm-T0x7vS5",
    "outputId": "5e3e8bfd-e31f-49bd-a0c0-bc731d4e0485"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.4873606)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cost_value(np.array([[0.3, 0.1, 0.3]]), np.array([[0.1, 0.1, 0.4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3362,
     "status": "ok",
     "timestamp": 1571387754653,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "3gl8U5XY7xC0",
    "outputId": "3405ecae-efb9-4bb7-88ba-3a7fad90097d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.69814624)"
      ]
     },
     "execution_count": 43,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cost_value(output, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbrreV7A70cr"
   },
   "source": [
    "## 4.5 Computing gradients with backpropagation\n",
    "\n",
    "Our neural network is dumb, we have been able to compute a loss. Now we need to understand where this loss is coming from, do we have dumber neurons than others ? For that we need to compute the gradient of the loss for each parameter.\n",
    "\n",
    "For that, we start from the loss at the output layer. We compute the derivatives of each neuron in order to update the weights and we go on until we reach the input layer.\n",
    "\n",
    "This process is called backpropagation.\n",
    "\n",
    "In NN, we calculate the gradient of the loss function in respect to parameters, but backpropagation can be used to calculate derivatives of any function. The essence of this algorithm is the recursive use of a chain rule known from differential calculusâ€Š-â€Šcalculate a derivative of functions created by assembling other functions, whose derivatives we already know. This process - for one network layer - is described by the following formulas. Looking at the formulas, it becomes obvious why we decided to remember the values of the A and Z matrices for intermediate layers in a forward step.\n",
    "\n",
    "$$\\boldsymbol{dW}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{W}^{[l]}} = \\frac{1}{m} \\boldsymbol{dZ}^{[l]} \\boldsymbol{A}^{[l-1] T}$$\n",
    "\n",
    "$$\\boldsymbol{db}^{[l]} = \\frac{\\partial L }{\\partial \\boldsymbol{b}^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} \\boldsymbol{dZ}^{[l](i)}$$\n",
    "\n",
    "$$\\boldsymbol{dA}^{[l-1]} = \\frac{\\partial L }{\\partial \\boldsymbol{A}^{[l-1]}} = \\boldsymbol{W}^{[l] T} \\boldsymbol{dZ}^{[l]}$$\n",
    "\n",
    "$$\\boldsymbol{dZ}^{[l]} = \\boldsymbol{dA}^{[l]} * g'(\\boldsymbol{Z}^{[l]})$$\n",
    "\n",
    "\n",
    "So first, we need to compute the derivatives of the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GbWxvcfq71Do"
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itocFH7i73Tq"
   },
   "source": [
    "Let's backpropagate now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EiPkz34875Fq"
   },
   "outputs": [],
   "source": [
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    # number of examples\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # selection of activation function\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    # calculation of the activation function derivative\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    \n",
    "    # derivative of the matrix W\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    # derivative of the vector b\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    # derivative of the matrix A_prev\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MTLsop0977l_"
   },
   "outputs": [],
   "source": [
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    \n",
    "    # number of examples\n",
    "    m = Y.shape[1]\n",
    "    # a hack ensuring the same shape of the prediction vector and labels vector\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "    \n",
    "    # initiation of gradient descent algorithm\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        # we number network layers from 1\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        # extraction of the activation function for the current layer\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        \n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yS-OfG3a7-_l"
   },
   "source": [
    "## 4.6 Updating parameters - Teaching\n",
    "\n",
    "Now that we know the gradient of the loss according to all neurons, we can teach the model, by updating the weights according to their gradients and a given learning rate.\n",
    "\n",
    "This update parameter is simply done here with gradient descent. More complex optimizers exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRXYTVB28BSB"
   },
   "outputs": [],
   "source": [
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "\n",
    "    # iteration over network layers\n",
    "    for layer_idx, layer in enumerate(nn_architecture, 1):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "im_4MsKQ8Dj1"
   },
   "source": [
    "## 4.7 Assembling bricks - Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_7Ib6amO8GG2"
   },
   "outputs": [],
   "source": [
    "def train(X, Y, nn_architecture, epochs=1000, learning_rate=0.1, verbose=False, callback=None):\n",
    "    # initiation of neural net parameters\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    # initiation of lists storing the history \n",
    "    # of metrics calculated during the learning process \n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    # performing calculations for subsequent iterations\n",
    "    for i in range(epochs):\n",
    "        # step forward\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        \n",
    "        # calculating metrics and saving them in history\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        # step backward - calculating gradient\n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        # updating model state\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "        if(i % 200 == 0):\n",
    "            if(verbose):\n",
    "                print(\"Iteration: {:05} - cost: {:.5f} - accuracy: {:.5f}\".format(i, cost, accuracy))\n",
    "            if(callback is not None):\n",
    "                callback(i, params_values)\n",
    "            \n",
    "    return params_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOGPzdAo8LtR"
   },
   "source": [
    "## 4.8 Let's try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j7fmDcHV8X1D"
   },
   "source": [
    "####  Archi 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 226903,
     "status": "ok",
     "timestamp": 1571388052370,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "gsH9TwYa8M1O",
    "outputId": "eb0afbb8-8c46-4d0c-82c8-0ab4e3515b79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - cost: 0.69120 - accuracy: 0.66089\n",
      "Iteration: 00200 - cost: 0.64040 - accuracy: 0.66085\n",
      "Iteration: 00400 - cost: 0.64035 - accuracy: 0.66085\n",
      "Iteration: 00600 - cost: 0.64028 - accuracy: 0.66085\n",
      "Iteration: 00800 - cost: 0.64016 - accuracy: 0.66085\n",
      "Iteration: 01000 - cost: 0.63994 - accuracy: 0.66085\n",
      "Iteration: 01200 - cost: 0.63950 - accuracy: 0.66085\n",
      "Iteration: 01400 - cost: 0.63850 - accuracy: 0.66085\n",
      "Iteration: 01600 - cost: 0.63565 - accuracy: 0.66085\n",
      "Iteration: 01800 - cost: 0.62524 - accuracy: 0.66085\n",
      "Iteration: 02000 - cost: 0.58357 - accuracy: 0.67122\n",
      "Iteration: 02200 - cost: 0.51381 - accuracy: 0.76037\n",
      "Iteration: 02400 - cost: 0.48476 - accuracy: 0.76829\n",
      "Iteration: 02600 - cost: 0.47759 - accuracy: 0.77224\n",
      "Iteration: 02800 - cost: 0.47555 - accuracy: 0.77312\n",
      "Iteration: 03000 - cost: 0.47480 - accuracy: 0.77308\n",
      "Iteration: 03200 - cost: 0.47447 - accuracy: 0.77362\n",
      "Iteration: 03400 - cost: 0.47429 - accuracy: 0.77337\n",
      "Iteration: 03600 - cost: 0.47420 - accuracy: 0.77337\n",
      "Iteration: 03800 - cost: 0.47413 - accuracy: 0.77358\n",
      "Iteration: 04000 - cost: 0.47406 - accuracy: 0.77312\n",
      "Iteration: 04200 - cost: 0.47399 - accuracy: 0.77320\n",
      "Iteration: 04400 - cost: 0.47389 - accuracy: 0.77324\n",
      "Iteration: 04600 - cost: 0.47379 - accuracy: 0.77358\n",
      "Iteration: 04800 - cost: 0.47370 - accuracy: 0.77349\n",
      "Iteration: 05000 - cost: 0.47361 - accuracy: 0.77374\n",
      "Iteration: 05200 - cost: 0.47354 - accuracy: 0.77391\n",
      "Iteration: 05400 - cost: 0.47346 - accuracy: 0.77379\n",
      "Iteration: 05600 - cost: 0.47338 - accuracy: 0.77349\n",
      "Iteration: 05800 - cost: 0.47330 - accuracy: 0.77349\n",
      "Iteration: 06000 - cost: 0.47321 - accuracy: 0.77362\n",
      "Iteration: 06200 - cost: 0.47311 - accuracy: 0.77366\n",
      "Iteration: 06400 - cost: 0.47300 - accuracy: 0.77366\n",
      "Iteration: 06600 - cost: 0.47284 - accuracy: 0.77399\n",
      "Iteration: 06800 - cost: 0.47261 - accuracy: 0.77441\n",
      "Iteration: 07000 - cost: 0.47236 - accuracy: 0.77454\n",
      "Iteration: 07200 - cost: 0.47210 - accuracy: 0.77404\n",
      "Iteration: 07400 - cost: 0.47187 - accuracy: 0.77412\n",
      "Iteration: 07600 - cost: 0.47162 - accuracy: 0.77391\n",
      "Iteration: 07800 - cost: 0.47134 - accuracy: 0.77420\n",
      "Iteration: 08000 - cost: 0.47095 - accuracy: 0.77454\n",
      "Iteration: 08200 - cost: 0.47061 - accuracy: 0.77462\n",
      "Iteration: 08400 - cost: 0.47031 - accuracy: 0.77491\n",
      "Iteration: 08600 - cost: 0.47000 - accuracy: 0.77487\n",
      "Iteration: 08800 - cost: 0.46967 - accuracy: 0.77491\n",
      "Iteration: 09000 - cost: 0.46932 - accuracy: 0.77466\n",
      "Iteration: 09200 - cost: 0.46897 - accuracy: 0.77458\n",
      "Iteration: 09400 - cost: 0.46865 - accuracy: 0.77474\n",
      "Iteration: 09600 - cost: 0.46835 - accuracy: 0.77466\n",
      "Iteration: 09800 - cost: 0.46806 - accuracy: 0.77420\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "x_train_features = tfidf_vectorizer.fit_transform(x_train).toarray()\n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "params_values = train(np.transpose(x_train_features),\n",
    "                      np.transpose(y_train_array.reshape((y_train_array.shape[0], 1))),\n",
    "                      NN_ARCHITECTURE_0, epochs=10000, learning_rate=0.1,\n",
    "                      verbose=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1623,
     "status": "ok",
     "timestamp": 1571388779197,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "beaItph48Vfp",
    "outputId": "65816cb5-a3a7-4f92-fea3-2cc98c3f647e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7132855475912652\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.53      0.58      0.55      1837\n",
      "         1.0       0.81      0.77      0.79      4162\n",
      "\n",
      "    accuracy                           0.71      5999\n",
      "   macro avg       0.67      0.68      0.67      5999\n",
      "weighted avg       0.72      0.71      0.72      5999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_test_features = tfidf_vectorizer.fit_transform(x_test).toarray()\n",
    "\n",
    "y_test_array = np.array(y_test)\n",
    "\n",
    "output, _ = full_forward_propagation(np.transpose(x_test_features),\n",
    "                                     params_values,\n",
    "                                     NN_ARCHITECTURE_0)\n",
    "y_pred_architecture_0 = convert_prob_into_class(output)\n",
    "\n",
    "print('accuracy {}'.format(accuracy_score(np.squeeze(y_pred_architecture_0), y_test)))\n",
    "print(classification_report(np.squeeze(y_pred_architecture_0), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N7lF4GFo8ZYO"
   },
   "source": [
    "####  Archi 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 624118,
     "status": "ok",
     "timestamp": 1571389406493,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "WLEj5-ML8Z7v",
    "outputId": "e5f73fdf-ffc4-48ff-83e5-7d04e632d354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 00000 - cost: 0.70041 - accuracy: 0.33915\n",
      "Iteration: 00200 - cost: 0.64046 - accuracy: 0.66085\n",
      "Iteration: 00400 - cost: 0.64045 - accuracy: 0.66085\n",
      "Iteration: 00600 - cost: 0.64044 - accuracy: 0.66085\n",
      "Iteration: 00800 - cost: 0.64043 - accuracy: 0.66085\n",
      "Iteration: 01000 - cost: 0.64042 - accuracy: 0.66085\n",
      "Iteration: 01200 - cost: 0.64041 - accuracy: 0.66085\n",
      "Iteration: 01400 - cost: 0.64039 - accuracy: 0.66085\n",
      "Iteration: 01600 - cost: 0.64037 - accuracy: 0.66085\n",
      "Iteration: 01800 - cost: 0.64034 - accuracy: 0.66085\n",
      "Iteration: 02000 - cost: 0.64030 - accuracy: 0.66085\n",
      "Iteration: 02200 - cost: 0.64024 - accuracy: 0.66085\n",
      "Iteration: 02400 - cost: 0.64017 - accuracy: 0.66085\n",
      "Iteration: 02600 - cost: 0.64005 - accuracy: 0.66085\n",
      "Iteration: 02800 - cost: 0.63988 - accuracy: 0.66085\n",
      "Iteration: 03000 - cost: 0.63959 - accuracy: 0.66085\n",
      "Iteration: 03200 - cost: 0.63903 - accuracy: 0.66085\n",
      "Iteration: 03400 - cost: 0.63770 - accuracy: 0.66085\n",
      "Iteration: 03600 - cost: 0.63298 - accuracy: 0.66085\n",
      "Iteration: 03800 - cost: 0.59283 - accuracy: 0.66085\n",
      "Iteration: 04000 - cost: 0.49594 - accuracy: 0.76558\n",
      "Iteration: 04200 - cost: 0.47949 - accuracy: 0.77041\n",
      "Iteration: 04400 - cost: 0.47591 - accuracy: 0.77216\n",
      "Iteration: 04600 - cost: 0.47456 - accuracy: 0.77362\n",
      "Iteration: 04800 - cost: 0.47371 - accuracy: 0.77412\n",
      "Iteration: 05000 - cost: 0.47312 - accuracy: 0.77383\n",
      "Iteration: 05200 - cost: 0.47265 - accuracy: 0.77412\n",
      "Iteration: 05400 - cost: 0.47226 - accuracy: 0.77466\n",
      "Iteration: 05600 - cost: 0.47191 - accuracy: 0.77429\n",
      "Iteration: 05800 - cost: 0.47156 - accuracy: 0.77420\n",
      "Iteration: 06000 - cost: 0.47116 - accuracy: 0.77429\n",
      "Iteration: 06200 - cost: 0.47072 - accuracy: 0.77458\n",
      "Iteration: 06400 - cost: 0.47017 - accuracy: 0.77466\n",
      "Iteration: 06600 - cost: 0.46957 - accuracy: 0.77470\n",
      "Iteration: 06800 - cost: 0.46941 - accuracy: 0.77408\n",
      "Iteration: 07000 - cost: 0.46941 - accuracy: 0.77374\n",
      "Iteration: 07200 - cost: 0.46845 - accuracy: 0.77387\n",
      "Iteration: 07400 - cost: 0.46746 - accuracy: 0.77387\n",
      "Iteration: 07600 - cost: 0.46763 - accuracy: 0.77404\n",
      "Iteration: 07800 - cost: 0.46725 - accuracy: 0.77479\n",
      "Iteration: 08000 - cost: 0.46434 - accuracy: 0.77487\n",
      "Iteration: 08200 - cost: 0.46505 - accuracy: 0.77654\n",
      "Iteration: 08400 - cost: 0.46427 - accuracy: 0.77612\n",
      "Iteration: 08600 - cost: 0.46173 - accuracy: 0.77775\n",
      "Iteration: 08800 - cost: 0.46170 - accuracy: 0.77762\n",
      "Iteration: 09000 - cost: 0.46060 - accuracy: 0.77825\n",
      "Iteration: 09200 - cost: 0.45829 - accuracy: 0.77966\n",
      "Iteration: 09400 - cost: 0.45916 - accuracy: 0.77895\n",
      "Iteration: 09600 - cost: 0.45772 - accuracy: 0.77995\n",
      "Iteration: 09800 - cost: 0.45747 - accuracy: 0.78025\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "x_train_features = tfidf_vectorizer.fit_transform(x_train).toarray()\n",
    "\n",
    "y_train_array = np.array(y_train)\n",
    "\n",
    "params_values_1 = train(np.transpose(x_train_features),\n",
    "                      np.transpose(y_train_array.reshape((y_train_array.shape[0], 1))),\n",
    "                      NN_ARCHITECTURE_1, epochs=10000, learning_rate=0.1,\n",
    "                      verbose=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 806,
     "status": "ok",
     "timestamp": 1571391159059,
     "user": {
      "displayName": "Marie Vachelard",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mBn-10cnCca1iyV0nxvMecKnK0XyPJ-qq4bBdJ4=s64",
      "userId": "06987863607739470266"
     },
     "user_tz": -120
    },
    "id": "V_kxbswt8b4y",
    "outputId": "61b01158-40ef-4cb4-da4b-3a0a412d2f6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7202867144524088\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.42      0.62      0.50      1341\n",
      "         1.0       0.87      0.75      0.81      4658\n",
      "\n",
      "    accuracy                           0.72      5999\n",
      "   macro avg       0.64      0.68      0.65      5999\n",
      "weighted avg       0.77      0.72      0.74      5999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_test_features = tfidf_vectorizer.fit_transform(x_test).toarray()\n",
    "\n",
    "y_test_array = np.array(y_test)\n",
    "\n",
    "output, _ = full_forward_propagation(np.transpose(x_test_features),\n",
    "                                     params_values_1,\n",
    "                                     NN_ARCHITECTURE_1)\n",
    "y_pred_architecture_1 = convert_prob_into_class(output)\n",
    "\n",
    "print('accuracy {}'.format(accuracy_score(np.squeeze(y_pred_architecture_1), y_test)))\n",
    "print(classification_report(np.squeeze(y_pred_architecture_1), y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Deep Learning session 1 - correction.ipynb",
   "provenance": [
    {
     "file_id": "15ZVpnFVFCtCC0VOFtyLowuQxMZsjnuLH",
     "timestamp": 1575532890553
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
